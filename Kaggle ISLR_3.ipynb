{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b08a64a",
   "metadata": {
    "papermill": {
     "duration": 0.005349,
     "end_time": "2023-03-14T22:47:32.283744",
     "exception": false,
     "start_time": "2023-03-14T22:47:32.278395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color: #0F092D; padding: 20px 0;\">\n",
    "    <h2 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-transform: none; letter-spacing: 2px; color: #FC796D; background-color: #0F092D; font-variant: small-caps;\">\n",
    "      <center><img style=\"padding: 20px 0 0 0;\" src=\"https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png\" alt=\"Google Logo\"></center><br>Isolated Sign Language Recognition<br><br><span style = \"font-size: 20px; color: white;\"></span></h2>\n",
    "    \n",
    "  <center><img src=\"https://cdn-cbkob.nitrocdn.com/TiGMibPGMREAJjbFYNLfxxdkUjUGroSw/assets/images/optimized/rev-22fc791/wp-content/uploads/2022/12/sign-language.jpg\" width=100% alt=\"asl banner\"></center>\n",
    "<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: white;\">CREATED BY:  SARASWATI TIWARI</h5><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80fccd",
   "metadata": {
    "papermill": {
     "duration": 0.003749,
     "end_time": "2023-03-14T22:47:32.291686",
     "exception": false,
     "start_time": "2023-03-14T22:47:32.287937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The goal of this competition is to classify isolated American Sign Language (ASL) signs. I will create a TensorFlow Lite model trained on labeled landmark data extracted using the MediaPipe Holistic Solution.**\n",
    "\n",
    "**My work may improve the ability of PopSign to help relatives of deaf children learn basic signs and communicate better with their loved ones.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190e673",
   "metadata": {
    "papermill": {
     "duration": 0.003869,
     "end_time": "2023-03-14T22:47:32.299445",
     "exception": false,
     "start_time": "2023-03-14T22:47:32.295576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Why TensorFlow Lite**\n",
    "\n",
    "To allow the ML model to run on device in an attempt to limit latency inside the game, PopSign doesn’t send user videos to the cloud. Therefore, all inference must be done on the phone itself. PopSign is building its recognition pipeline on top of TensorFlow Lite, which runs on both Android and iOS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f879f0",
   "metadata": {
    "papermill": {
     "duration": 0.003757,
     "end_time": "2023-03-14T22:47:32.307138",
     "exception": false,
     "start_time": "2023-03-14T22:47:32.303381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**PopSign** *is an app developed by the Georgia Institute of Technology and the National Technical Institute for the Deaf at Rochester Institute of Technology. The app is available in beta on Android and iOS.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57196bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:32.317334Z",
     "iopub.status.busy": "2023-03-14T22:47:32.316595Z",
     "iopub.status.idle": "2023-03-14T22:47:39.766308Z",
     "shell.execute_reply": "2023-03-14T22:47:39.764838Z"
    },
    "papermill": {
     "duration": 7.457565,
     "end_time": "2023-03-14T22:47:39.768627",
     "exception": false,
     "start_time": "2023-03-14T22:47:32.311062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... PIP INSTALLS STARTING ...\n",
      "\n",
      "\n",
      "... PIP INSTALLS COMPLETE ...\n",
      "\n",
      "\n",
      "... IMPORTS STARTING ...\n",
      "\n",
      "\n",
      "\tVERSION INFORMATION\n",
      "\t\t– TENSORFLOW VERSION: 2.11.0\n",
      "\t\t– NUMPY VERSION: 1.21.6\n",
      "\t\t– SKLEARN VERSION: 1.0.2\n",
      "\n",
      "\n",
      "... IMPORTS COMPLETE ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n... PIP INSTALLS STARTING ...\\n\")\n",
    "print(\"\\n... PIP INSTALLS COMPLETE ...\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n... IMPORTS STARTING ...\\n\")\n",
    "print(\"\\n\\tVERSION INFORMATION\")\n",
    "# Competition Specific Imports (You'll see why we need these later)\n",
    "# mediapipe above\n",
    "\n",
    "# Machine Learning and Data Science Imports (basics)\n",
    "import tensorflow as tf; print(f\"\\t\\t– TENSORFLOW VERSION: {tf.__version__}\");\n",
    "# import tensorflow_io as tfio; print(f\"\\t\\t– TENSORFLOW-IO VERSION: {tfio.__version__}\");\n",
    "import pandas as pd; pd.options.mode.chained_assignment = None; pd.set_option('display.max_columns', None);\n",
    "import numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\n",
    "import sklearn; print(f\"\\t\\t– SKLEARN VERSION: {sklearn.__version__}\");\n",
    "\n",
    "# Built-In Imports (mostly don't worry about these)\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "from glob import glob\n",
    "import Levenshtein\n",
    "import warnings\n",
    "import requests\n",
    "import hashlib\n",
    "import imageio\n",
    "import IPython\n",
    "import sklearn\n",
    "import urllib\n",
    "import zipfile\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import gzip\n",
    "import ast\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "print(\"\\n\\n... IMPORTS COMPLETE ...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4130d590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:39.778862Z",
     "iopub.status.busy": "2023-03-14T22:47:39.778236Z",
     "iopub.status.idle": "2023-03-14T22:47:39.783128Z",
     "shell.execute_reply": "2023-03-14T22:47:39.781995Z"
    },
    "papermill": {
     "duration": 0.012506,
     "end_time": "2023-03-14T22:47:39.785506",
     "exception": false,
     "start_time": "2023-03-14T22:47:39.773000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a8caae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:39.794893Z",
     "iopub.status.busy": "2023-03-14T22:47:39.794589Z",
     "iopub.status.idle": "2023-03-14T22:47:46.099273Z",
     "shell.execute_reply": "2023-03-14T22:47:46.097920Z"
    },
    "papermill": {
     "duration": 6.311971,
     "end_time": "2023-03-14T22:47:46.101651",
     "exception": false,
     "start_time": "2023-03-14T22:47:39.789680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': <KerasTensor: shape=(250,) dtype=float32 (created by layer 'tf_lite_model')>}\n",
      "{'outputs': <KerasTensor: shape=(250,) dtype=float32 (created by layer 'tf_lite_model')>}\n"
     ]
    }
   ],
   "source": [
    "models.append(tf.keras.models.load_model(\"/kaggle/input/gislr-lb-0-63-on-the-shoulders/models/final_model\"))\n",
    "weights.append(0.8)\n",
    "\n",
    "print(models[-1](tf.keras.Input(shape=(543, 3))))\n",
    "\n",
    "models.append(tf.keras.models.load_model(\"/kaggle/input/gislr-small-version-on-the-shoulders/models/final_model\"))\n",
    "weights.append(0.4)\n",
    "\n",
    "print(models[-1](tf.keras.Input(shape=(543, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224f6f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:46.112096Z",
     "iopub.status.busy": "2023-03-14T22:47:46.111793Z",
     "iopub.status.idle": "2023-03-14T22:47:46.116157Z",
     "shell.execute_reply": "2023-03-14T22:47:46.115030Z"
    },
    "papermill": {
     "duration": 0.012528,
     "end_time": "2023-03-14T22:47:46.118740",
     "exception": false,
     "start_time": "2023-03-14T22:47:46.106212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_model_path = \"/kaggle/input/asl-sign-detection-pytorch-lightning/tf_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b40afc87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:46.129322Z",
     "iopub.status.busy": "2023-03-14T22:47:46.128502Z",
     "iopub.status.idle": "2023-03-14T22:47:46.140677Z",
     "shell.execute_reply": "2023-03-14T22:47:46.139687Z"
    },
    "papermill": {
     "duration": 0.019675,
     "end_time": "2023-03-14T22:47:46.142755",
     "exception": false,
     "start_time": "2023-03-14T22:47:46.123080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "DROP_Z = False\n",
    "\n",
    "NUM_FRAMES = 15\n",
    "SEGMENTS = 3\n",
    "\n",
    "LEFT_HAND_OFFSET = 468\n",
    "POSE_OFFSET = LEFT_HAND_OFFSET + 21\n",
    "RIGHT_HAND_OFFSET = POSE_OFFSET + 33\n",
    "\n",
    "## average over the entire face, and the entire 'pose'\n",
    "averaging_sets = [[0, 468], [POSE_OFFSET, 33]]\n",
    "\n",
    "lip_landmarks = [\n",
    "    61,\n",
    "    185,\n",
    "    40,\n",
    "    39,\n",
    "    37,\n",
    "    0,\n",
    "    267,\n",
    "    269,\n",
    "    270,\n",
    "    409,\n",
    "    291,\n",
    "    146,\n",
    "    91,\n",
    "    181,\n",
    "    84,\n",
    "    17,\n",
    "    314,\n",
    "    405,\n",
    "    321,\n",
    "    375,\n",
    "    78,\n",
    "    191,\n",
    "    80,\n",
    "    81,\n",
    "    82,\n",
    "    13,\n",
    "    312,\n",
    "    311,\n",
    "    310,\n",
    "    415,\n",
    "    95,\n",
    "    88,\n",
    "    178,\n",
    "    87,\n",
    "    14,\n",
    "    317,\n",
    "    402,\n",
    "    318,\n",
    "    324,\n",
    "    308,\n",
    "]\n",
    "left_hand_landmarks = list(range(LEFT_HAND_OFFSET, LEFT_HAND_OFFSET + 21))\n",
    "right_hand_landmarks = list(range(RIGHT_HAND_OFFSET, RIGHT_HAND_OFFSET + 21))\n",
    "\n",
    "point_landmarks = [\n",
    "    item\n",
    "    for sublist in [lip_landmarks, left_hand_landmarks, right_hand_landmarks]\n",
    "    for item in sublist\n",
    "]\n",
    "\n",
    "LANDMARKS = len(point_landmarks) + len(averaging_sets)\n",
    "print(LANDMARKS)\n",
    "if DROP_Z:\n",
    "    INPUT_SHAPE = (NUM_FRAMES, LANDMARKS * 2)\n",
    "else:\n",
    "    INPUT_SHAPE = (NUM_FRAMES, LANDMARKS * 3)\n",
    "\n",
    "FLAT_INPUT_SHAPE = (INPUT_SHAPE[0] + 2 * (SEGMENTS + 1)) * INPUT_SHAPE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7aa44f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:46.152421Z",
     "iopub.status.busy": "2023-03-14T22:47:46.152159Z",
     "iopub.status.idle": "2023-03-14T22:47:46.159722Z",
     "shell.execute_reply": "2023-03-14T22:47:46.158662Z"
    },
    "papermill": {
     "duration": 0.015133,
     "end_time": "2023-03-14T22:47:46.162150",
     "exception": false,
     "start_time": "2023-03-14T22:47:46.147017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0):\n",
    "    return tf.reduce_sum(\n",
    "        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis\n",
    "    ) / tf.reduce_sum(\n",
    "        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_nan_std(x, axis=0):\n",
    "    d = x - tf_nan_mean(x, axis=axis)\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis))\n",
    "\n",
    "\n",
    "def flatten_means_and_stds(x, axis=0):\n",
    "    # Get means and stds\n",
    "    x_mean = tf_nan_mean(x, axis=0)\n",
    "    x_std = tf_nan_std(x, axis=0)\n",
    "\n",
    "    x_out = tf.concat([x_mean, x_std], axis=0)\n",
    "    x_out = tf.reshape(x_out, (1, INPUT_SHAPE[1] * 2))\n",
    "    x_out = tf.where(tf.math.is_finite(x_out), x_out, tf.zeros_like(x_out))\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4169a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:46.171890Z",
     "iopub.status.busy": "2023-03-14T22:47:46.171617Z",
     "iopub.status.idle": "2023-03-14T22:47:46.182485Z",
     "shell.execute_reply": "2023-03-14T22:47:46.181476Z"
    },
    "papermill": {
     "duration": 0.018143,
     "end_time": "2023-03-14T22:47:46.184590",
     "exception": false,
     "start_time": "2023-03-14T22:47:46.166447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureGenTF(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, x_in):\n",
    "        if DROP_Z:\n",
    "            x_in = x_in[:, :, 0:2]\n",
    "        x_list = [\n",
    "            tf.expand_dims(\n",
    "                tf_nan_mean(x_in[:, av_set[0] : av_set[0] + av_set[1], :], axis=1),\n",
    "                axis=1,\n",
    "            )\n",
    "            for av_set in averaging_sets\n",
    "        ]\n",
    "        x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n",
    "        x = tf.concat(x_list, 1)\n",
    "\n",
    "        x_padded = x\n",
    "        for i in range(SEGMENTS):\n",
    "            p0 = tf.where(\n",
    "                ((tf.shape(x_padded)[0] % SEGMENTS) > 0) & ((i % 2) != 0), 1, 0\n",
    "            )\n",
    "            p1 = tf.where(\n",
    "                ((tf.shape(x_padded)[0] % SEGMENTS) > 0) & ((i % 2) == 0), 1, 0\n",
    "            )\n",
    "            paddings = [[p0, p1], [0, 0], [0, 0]]\n",
    "            x_padded = tf.pad(x_padded, paddings, mode=\"SYMMETRIC\")\n",
    "        x_list = tf.split(x_padded, SEGMENTS)\n",
    "        x_list = [flatten_means_and_stds(_x, axis=0) for _x in x_list]\n",
    "\n",
    "        x_list.append(flatten_means_and_stds(x, axis=0))\n",
    "\n",
    "        ## Resize only dimension 0. Resize can't handle nan, so replace nan with that dimension's avg value to reduce impact.\n",
    "        x = tf.image.resize(\n",
    "            tf.where(tf.math.is_finite(x), x, tf_nan_mean(x, axis=0)),\n",
    "            [NUM_FRAMES, LANDMARKS],\n",
    "        )\n",
    "        x = tf.reshape(x, (1, INPUT_SHAPE[0] * INPUT_SHAPE[1]))\n",
    "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "        x_list.append(x)\n",
    "        x = tf.concat(x_list, axis=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd059e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:46.194310Z",
     "iopub.status.busy": "2023-03-14T22:47:46.194037Z",
     "iopub.status.idle": "2023-03-14T22:47:47.128778Z",
     "shell.execute_reply": "2023-03-14T22:47:47.127561Z"
    },
    "papermill": {
     "duration": 0.942685,
     "end_time": "2023-03-14T22:47:47.131683",
     "exception": false,
     "start_time": "2023-03-14T22:47:46.188998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ASLInferModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, tf_model_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_gen = FeatureGenTF()\n",
    "        self.model = tf.saved_model.load(tf_model_path)\n",
    "        self.feature_gen.trainable = False\n",
    "        self.model.trainable = False\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name=\"inputs\")\n",
    "        ]\n",
    "    )\n",
    "    def call(self, inputs):\n",
    "        features = self.feature_gen(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs = self.model(input=features)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "mytfmodel = ASLInferModel(tf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eff029c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:47.144659Z",
     "iopub.status.busy": "2023-03-14T22:47:47.143555Z",
     "iopub.status.idle": "2023-03-14T22:47:47.158403Z",
     "shell.execute_reply": "2023-03-14T22:47:47.157388Z"
    },
    "papermill": {
     "duration": 0.024165,
     "end_time": "2023-03-14T22:47:47.161269",
     "exception": false,
     "start_time": "2023-03-14T22:47:47.137104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFLiteEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights):\n",
    "        super(TFLiteEnsemble, self).__init__()\n",
    "        self.weight_list = weights\n",
    "        self.models = models\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')])\n",
    "    def call(self, inputs):\n",
    "        x = tf.cast(inputs, dtype=tf.float32)\n",
    "        ## The commented out line might work better for KFold, I haven't tested it yet.\n",
    "        # model_outputs = [model(x) for model in self.models]\n",
    "\n",
    "        ## This version with '.popitem()[1]' requires that the 'model(inputs)' call always returns a dict with only one key: value pair.\n",
    "        ## It is a workaround due to different models having dict['output'] vs dict['outputs']\n",
    "        ## Second workaround: tf.reshape to handle either (1, 250) or (250)\n",
    "        model_outputs = [tf.reshape(model(x).popitem()[1], [-1]) for model in self.models]\n",
    "        ## NOTE: this assumes weights add to 1.0!\n",
    "        outputs = tf.add_n([tf.multiply(w, inp) for (w, inp) in zip(self.weight_list, model_outputs)])\n",
    "\n",
    "        # Return a dictionary with the output tensor\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "ensemble_model = TFLiteEnsemble(models, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3086a3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:47.172645Z",
     "iopub.status.busy": "2023-03-14T22:47:47.172308Z",
     "iopub.status.idle": "2023-03-14T22:47:58.789408Z",
     "shell.execute_reply": "2023-03-14T22:47:58.787630Z"
    },
    "papermill": {
     "duration": 11.626487,
     "end_time": "2023-03-14T22:47:58.792794",
     "exception": false,
     "start_time": "2023-03-14T22:47:47.166307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model.tflite (deflated 7%)\r\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(ensemble_model)\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "!zip submission.zip model.tflite\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c62ad5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:58.804268Z",
     "iopub.status.busy": "2023-03-14T22:47:58.803570Z",
     "iopub.status.idle": "2023-03-14T22:47:58.809941Z",
     "shell.execute_reply": "2023-03-14T22:47:58.808902Z"
    },
    "papermill": {
     "duration": 0.014243,
     "end_time": "2023-03-14T22:47:58.811985",
     "exception": false,
     "start_time": "2023-03-14T22:47:58.797742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0ce6d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:58.822318Z",
     "iopub.status.busy": "2023-03-14T22:47:58.822052Z",
     "iopub.status.idle": "2023-03-14T22:47:58.842324Z",
     "shell.execute_reply": "2023-03-14T22:47:58.841427Z"
    },
    "papermill": {
     "duration": 0.027996,
     "end_time": "2023-03-14T22:47:58.844455",
     "exception": false,
     "start_time": "2023-03-14T22:47:58.816459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manually add decoder\n",
    "decoder = {v:k for k, v in {'tv': 0, 'after': 1, 'airplane': 2, 'all': 3, 'alligator': 4, 'animal': 5, 'another': 6, 'any': 7, 'apple': 8, 'arm': 9, 'aunt': 10, 'awake': 11, 'backyard': 12, 'bad': 13, 'balloon': 14, \n",
    "           'bath': 15, 'because': 16, 'bed': 17, 'bedroom': 18, 'bee': 19, 'before': 20, 'beside': 21, 'better': 22, 'bird': 23, 'black': 24, 'blow': 25, 'blue': 26, 'boat': 27, 'book': 28, 'boy': 29, \n",
    "           'brother': 30, 'brown': 31, 'bug': 32, 'bye': 33, 'callonphone': 34, 'can': 35, 'car': 36, 'carrot': 37, 'cat': 38, 'cereal': 39, 'chair': 40, 'cheek': 41, 'child': 42, 'chin': 43, 'chocolate': 44, \n",
    "           'clean': 45, 'close': 46, 'closet': 47, 'cloud': 48, 'clown': 49, 'cow': 50, 'cowboy': 51, 'cry': 52, 'cut': 53, 'cute': 54, 'dad': 55, 'dance': 56, 'dirty': 57, 'dog': 58, 'doll': 59, 'donkey': 60, \n",
    "           'down': 61, 'drawer': 62, 'drink': 63, 'drop': 64, 'dry': 65, 'dryer': 66, 'duck': 67, 'ear': 68, 'elephant': 69, 'empty': 70, 'every': 71, 'eye': 72, 'face': 73, 'fall': 74, 'farm': 75, 'fast': 76, \n",
    "           'feet': 77, 'find': 78, 'fine': 79, 'finger': 80, 'finish': 81, 'fireman': 82, 'first': 83, 'fish': 84, 'flag': 85, 'flower': 86, 'food': 87, 'for': 88, 'frenchfries': 89, 'frog': 90, 'garbage': 91, \n",
    "           'gift': 92, 'giraffe': 93, 'girl': 94, 'give': 95, 'glasswindow': 96, 'go': 97, 'goose': 98, 'grandma': 99, 'grandpa': 100, 'grass': 101, 'green': 102, 'gum': 103, 'hair': 104, 'happy': 105, 'hat': 106, \n",
    "           'hate': 107, 'have': 108, 'haveto': 109, 'head': 110, 'hear': 111, 'helicopter': 112, 'hello': 113, 'hen': 114, 'hesheit': 115, 'hide': 116, 'high': 117, 'home': 118, 'horse': 119, 'hot': 120, 'hungry': 121, \n",
    "           'icecream': 122, 'if': 123, 'into': 124, 'jacket': 125, 'jeans': 126, 'jump': 127, 'kiss': 128, 'kitty': 129, 'lamp': 130, 'later': 131, 'like': 132, 'lion': 133, 'lips': 134, 'listen': 135, 'look': 136, \n",
    "           'loud': 137, 'mad': 138, 'make': 139, 'man': 140, 'many': 141, 'milk': 142, 'minemy': 143, 'mitten': 144, 'mom': 145, 'moon': 146, 'morning': 147, 'mouse': 148, 'mouth': 149, 'nap': 150, 'napkin': 151, \n",
    "           'night': 152, 'no': 153, 'noisy': 154, 'nose': 155, 'not': 156, 'now': 157, 'nuts': 158, 'old': 159, 'on': 160, 'open': 161, 'orange': 162, 'outside': 163, 'owie': 164, 'owl': 165, 'pajamas': 166, 'pen': 167,\n",
    "           'pencil': 168, 'penny': 169, 'person': 170, 'pig': 171, 'pizza': 172, 'please': 173, 'police': 174, 'pool': 175, 'potty': 176, 'pretend': 177, 'pretty': 178, 'puppy': 179, 'puzzle': 180, 'quiet': 181, 'radio': 182, \n",
    "           'rain': 183, 'read': 184, 'red': 185, 'refrigerator': 186, 'ride': 187, 'room': 188, 'sad': 189, 'same': 190, 'say': 191, 'scissors': 192, 'see': 193, 'shhh': 194, 'shirt': 195, 'shoe': 196, 'shower': 197, \n",
    "           'sick': 198, 'sleep': 199, 'sleepy': 200, 'smile': 201, 'snack': 202, 'snow': 203, 'stairs': 204, 'stay': 205, 'sticky': 206, 'store': 207, 'story': 208, 'stuck': 209, 'sun': 210, 'table': 211, 'talk': 212, \n",
    "           'taste': 213, 'thankyou': 214, 'that': 215, 'there': 216, 'think': 217, 'thirsty': 218, 'tiger': 219, 'time': 220, 'tomorrow': 221, 'tongue': 222, 'tooth': 223, 'toothbrush': 224, 'touch': 225, 'toy': 226, \n",
    "           'tree': 227, 'uncle': 228, 'underwear': 229, 'up': 230, 'vacuum': 231, 'wait': 232, 'wake': 233, 'water': 234, 'wet': 235, 'weus': 236, 'where': 237, 'white': 238, 'who': 239, 'why': 240, 'will': 241, \n",
    "           'wolf': 242, 'yellow': 243, 'yes': 244, 'yesterday': 245, 'yourself': 246, 'yucky': 247, 'zebra': 248, 'zipper': 249}.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c98073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T22:47:58.855188Z",
     "iopub.status.busy": "2023-03-14T22:47:58.854321Z",
     "iopub.status.idle": "2023-03-14T22:48:09.649029Z",
     "shell.execute_reply": "2023-03-14T22:48:09.647037Z"
    },
    "papermill": {
     "duration": 10.803278,
     "end_time": "2023-03-14T22:48:09.652207",
     "exception": false,
     "start_time": "2023-03-14T22:47:58.848929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflite-runtime\r\n",
      "  Downloading tflite_runtime-2.11.0-cp37-cp37m-manylinux2014_x86_64.whl (2.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tflite-runtime) (1.21.6)\r\n",
      "Installing collected packages: tflite-runtime\r\n",
      "Successfully installed tflite-runtime-2.11.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mPRED :  blow\n",
      "GT   :  blow\n"
     ]
    }
   ],
   "source": [
    "!pip install tflite-runtime\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "interpreter = tflite.Interpreter(\"model.tflite\")\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "output = prediction_fn(inputs=load_relevant_data_subset(f'/kaggle/input/asl-signs/{pd.read_csv(\"/kaggle/input/asl-signs/train.csv\").path[0]}'))\n",
    "sign = np.argmax(output[\"outputs\"])\n",
    "\n",
    "print(\"PRED : \", decoder[sign])\n",
    "print(\"GT   : \", pd.read_csv(\"/kaggle/input/asl-signs/train.csv\").sign[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f60097",
   "metadata": {
    "papermill": {
     "duration": 0.004716,
     "end_time": "2023-03-14T22:48:09.662372",
     "exception": false,
     "start_time": "2023-03-14T22:48:09.657656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Every day, 33 babies are born with permanent hearing loss in the U.S.**\n",
    "\n",
    "Around 90% of which are born to hearing parents many of which may not know American Sign Language. (kdhe.ks.gov, deafchildren.org) Without sign language, deaf babies are at risk of Language Deprivation Syndrome. This syndrome is characterized by a lack of access to naturally occurring language acquisition during their critical language-learning years. It can cause serious impacts on different aspects of their lives, such as relationships, education, and employment.\n",
    "\n",
    "**Learning sign language is challenging.**\n",
    "\n",
    "Learning American Sign Language is as difficult for English speakers as learning Japanese. (jstor.org) It takes time and resources, which many parents don't have. They want to learn sign language, but it's hard when they are working long hours just to make ends meet. And even if they find the time and money for classes, the classes are often far away.\n",
    "\n",
    "**Games can help.**\n",
    "\n",
    "PopSign is a smartphone game app that makes learning American Sign Language fun, interactive, and accessible. Players match videos of ASL signs with bubbles containing written English words to pop them.\n",
    "PopSign is designed to help parents with deaf children learn ASL, but it's open to anyone who wants to learn sign language vocabulary. By adding a sign language recognizer from this competition, PopSign players will be able to sign the type of bubble they want to shoot, providing the player with the opportunity to practice the sign themselves instead of just watching videos of other people signing.\n",
    "\n",
    "**You can help connect deaf children and their parents.**\n",
    "\n",
    "By training a sign language recognizer for PopSign, you can help make the game more interactive and improve the learning and confidence of players who want to learn sign language to communicate with their loved ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 49.744507,
   "end_time": "2023-03-14T22:48:13.053403",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-14T22:47:23.308896",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
